{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import DeltaHedge\n",
    "from discrete_environments import DiscreteEnv, DiscreteEnv2\n",
    "from data_generators import GBM_Generator\n",
    "import utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from machin.frame.algorithms import DQN\n",
    "from machin.frame.algorithms.dqn import DQN\n",
    "from machin.utils.logging import default_logger as logger\n",
    "import torch as t\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_dim = 3\n",
    "action_num = 101\n",
    "max_episodes = 10000\n",
    "solved_reward = 0\n",
    "solved_repeat = 7\n",
    "\n",
    "load_weights = False\n",
    "save_weights = False\n",
    "weights_dir = \"./DQN_weights/\"\n",
    "\n",
    "# model definition\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_num):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 10)\n",
    "        self.bn1 = nn.LayerNorm(10, elementwise_affine=False)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.bn2 = nn.LayerNorm(10, elementwise_affine=False)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.bn3 = nn.LayerNorm(10, elementwise_affine=False)\n",
    "        self.fc4 = nn.Linear(10, action_num)\n",
    "\n",
    "    def forward(self, some_state):\n",
    "        a = self.fc1(some_state)\n",
    "        a = t.relu(self.bn1(a))\n",
    "        a = self.fc2(a)\n",
    "        a = t.relu(self.bn2(a))\n",
    "        a = self.fc3(a)\n",
    "        a = t.relu(self.bn3(a))\n",
    "        a = self.fc4(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 100\n",
    "\n",
    "# Annualized\n",
    "sigma = 0.01*np.sqrt(250) # 1% vol per day\n",
    "r = 0.0\n",
    "\n",
    "freq = 0.2 # corresponds to trading freq of 5x per day\n",
    "ttm = 50\n",
    "kappa = 0.1\n",
    "cost_multiplier = 0.0\n",
    "gamma = 0.88\n",
    "\n",
    "generator = GBM_Generator(S0, r, sigma, freq)\n",
    "\n",
    "env_args = {\n",
    "    \"generator\" : generator,\n",
    "    \"ttm\" : ttm,\n",
    "    \"kappa\" : kappa,\n",
    "    \"cost_multiplier\" : cost_multiplier,\n",
    "    \"testing\" : False\n",
    "}\n",
    "\n",
    "env = DiscreteEnv2(**env_args)\n",
    "\n",
    "# 1 epoch = 3000 episodes = 150k time-steps\n",
    "#epoch = 150000\n",
    "#n_epochs_per_update = 5\n",
    "#n_updates = int(epoch * n_epochs_per_update / batch_size)\n",
    "\n",
    "batch_size = 32\n",
    "final_eps = 0.05\n",
    "eps_decay = np.exp(np.log(final_eps)/(max_episodes*ttm))\n",
    "\n",
    "\n",
    "q_net = QNet(observe_dim, action_num)\n",
    "q_net_t = QNet(observe_dim, action_num)\n",
    "dqn = DQN(q_net, q_net_t,\n",
    "            t.optim.Adam,\n",
    "            nn.MSELoss(reduction='sum'),\n",
    "            visualize=False,\n",
    "            learning_rate=1e-5,\n",
    "            batch_size=batch_size,\n",
    "            discount=gamma,\n",
    "            gradient_max=1.0,\n",
    "            epsilon_decay=eps_decay,\n",
    "            replay_size=750000,\n",
    "            update_rate=1.0,\n",
    "            mode=\"fixed_target\"\n",
    "            )\n",
    "\n",
    "if load_weights:\n",
    "    dqn.load(weights_dir, {\"qnet_target\" : \"qnt\"})\n",
    "\n",
    "episode, step, reward_fulfilled = 0, 0, 0\n",
    "smoothed_total_reward = 0\n",
    "terminal = False\n",
    "eps = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while episode < max_episodes:\n",
    "    episode += 1\n",
    "    total_reward = 0\n",
    "    terminal = False\n",
    "    step = 0\n",
    "    state = t.tensor(env.reset(), dtype=t.float32).view(1, observe_dim)\n",
    "    ep_list = []\n",
    "    while not terminal:\n",
    "        step += 1\n",
    "        with t.no_grad():\n",
    "            old_state = state\n",
    "            # agent model inference\n",
    "            action = dqn.act_discrete_with_noise(\n",
    "                {\"some_state\": old_state},\n",
    "                use_target=True,\n",
    "                decay_epsilon=True\n",
    "            )\n",
    "            #eps = eps*dqn.epsilon_decay\n",
    "            state, reward, terminal, _ = env.step(action.item())\n",
    "            state = t.tensor(state, dtype=t.float32).view(1, observe_dim)\n",
    "            total_reward += reward\n",
    "            clipped_reward = np.clip(reward,-10.0,10.0)\n",
    "\n",
    "            ep_list.append({\n",
    "                \"state\": {\"some_state\": old_state},\n",
    "                \"action\": {\"action\": action},\n",
    "                \"next_state\": {\"some_state\": state},\n",
    "                \"reward\": clipped_reward,\n",
    "                \"terminal\": terminal\n",
    "            })\n",
    "\n",
    "    dqn.store_episode(ep_list)\n",
    "    # update target net\n",
    "    if (episode > 500):\n",
    "        for _ in range(50):\n",
    "            dqn.update(update_value=True,update_target=False)\n",
    "\n",
    "    # one update of target net\n",
    "    if (episode % 1000 == 0):\n",
    "        dqn.update(update_value=False,update_target=True)\n",
    "\n",
    "    # show reward\n",
    "    smoothed_total_reward = (smoothed_total_reward * 0.9 +\n",
    "                                total_reward * 0.1)\n",
    "    logger.info(\"Episode {} total reward={:.2f}\"\n",
    "                .format(episode, smoothed_total_reward))\n",
    "\n",
    "    if smoothed_total_reward > solved_reward:\n",
    "        reward_fulfilled += 1\n",
    "        if reward_fulfilled >= solved_repeat:\n",
    "            logger.info(\"Environment solved!\")\n",
    "            break\n",
    "    else:\n",
    "        reward_fulfilled = 0\n",
    "\n",
    "if save_weights:\n",
    "    dqn.save(weights_dir, {\"qnet_target\" : \"qnt\"}, version=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "generator = GBM_Generator(S0, r, sigma, freq, seed = 1234)\n",
    "env_args = {\n",
    "\"generator\" : generator,\n",
    "\"ttm\" : ttm,\n",
    "\"kappa\" : kappa,\n",
    "\"cost_multiplier\" : cost_multiplier,\n",
    "\"testing\" : True\n",
    "}\n",
    "\n",
    "env = DiscreteEnv2(**env_args)\n",
    "state = t.tensor(env.reset(), dtype=t.float32).view(1, observe_dim)\n",
    "terminal = False\n",
    "while not terminal:\n",
    "    with t.no_grad():\n",
    "        old_state = state\n",
    "        # agent model inference\n",
    "        action = dqn.act_discrete(\n",
    "            {\"some_state\": old_state},\n",
    "            use_target=True\n",
    "        )\n",
    "        state, reward, terminal, info = env.step(action.item())\n",
    "        state = t.tensor(state, dtype=t.float32).view(1, observe_dim)\n",
    "\n",
    "df = info['output']\n",
    "\n",
    "# delta hedge benchmark\n",
    "test_env_delta = DiscreteEnv(**env_args)\n",
    "delta_agent = DeltaHedge(r, sigma, S0)\n",
    "delta = delta_agent.test(test_env_delta)\n",
    "\n",
    "utils.plot_decisions(delta, df)\n",
    "utils.plot_pnl(delta, df)\n",
    "\n",
    "n_sim = 300\n",
    "generator = GBM_Generator(r = r, sigma = sigma, S0 = S0, freq = freq)\n",
    "env_args[\"generator\"] = generator\n",
    "env_args[\"testing\"] = True\n",
    "pnl_paths_dict, pnl_dict, tcosts_dict, ntrades_dict = utils.simulate_pnl_DQN(dqn, delta_agent, n_sim, env_args)\n",
    "utils.plot_pnl_hist(pnl_paths_dict, pnl_dict, tcosts_dict, ntrades_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a293d65eec8dde5de2981af85d84dd43bd8051257380932fcaaf2cf90c1085ab"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
