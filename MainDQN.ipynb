{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import ModelAverager, DeltaHedge, DRLAgent\n",
    "from discrete_environments import DiscreteEnv, DiscreteEnv2\n",
    "from data_generators import GBM_Generator, HestonGenerator\n",
    "import utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from machin.frame.algorithms import DQN\n",
    "from machin.frame.algorithms.dqn import DQN\n",
    "from machin.utils.logging import default_logger as logger\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "#env = gym.make(\"CartPole-v0\")\n",
    "observe_dim = 3\n",
    "action_num = 101\n",
    "max_episodes = 1000\n",
    "max_steps = 200\n",
    "solved_reward = 190\n",
    "solved_repeat = 5\n",
    "\n",
    "\n",
    "# model definition\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_num):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, action_num)\n",
    "\n",
    "    def forward(self, some_state):\n",
    "        a = t.relu(self.fc1(some_state))\n",
    "        a = t.relu(self.fc2(a))\n",
    "        return self.fc3(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 100\n",
    "\n",
    "# Annualized\n",
    "sigma = 0.01*np.sqrt(250) # 1% vol per day\n",
    "r = 0.0\n",
    "\n",
    "freq = 0.2 # corresponds to trading freq of 5x per day\n",
    "ttm = 50\n",
    "kappa = 0.1\n",
    "cost_multiplier = 0.0\n",
    "gamma = 0.99\n",
    "\n",
    "generator = GBM_Generator(S0, r, sigma, freq)\n",
    "\n",
    "env_args = {\n",
    "    \"generator\" : generator,\n",
    "    \"ttm\" : ttm,\n",
    "    \"kappa\" : kappa,\n",
    "    \"cost_multiplier\" : cost_multiplier,\n",
    "    \"testing\" : False\n",
    "}\n",
    "\n",
    "env = DiscreteEnv2(**env_args)\n",
    "#drl_env, _ = env.get_sb_env()\n",
    "\n",
    "q_net = QNet(observe_dim, action_num)\n",
    "q_net_t = QNet(observe_dim, action_num)\n",
    "dqn = DQN(q_net, q_net_t,\n",
    "            t.optim.Adam,\n",
    "            nn.MSELoss(reduction='sum'))\n",
    "\n",
    "episode, step, reward_fulfilled = 0, 0, 0\n",
    "smoothed_total_reward = 0\n",
    "terminal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while episode < max_episodes:\n",
    "        episode += 1\n",
    "        total_reward = 0\n",
    "        terminal = False\n",
    "        step = 0\n",
    "        state = t.tensor(env.reset(), dtype=t.float32).view(1, observe_dim)\n",
    "        ep_list = []\n",
    "        while not terminal and step <= max_steps:\n",
    "            step += 1\n",
    "            with t.no_grad():\n",
    "                old_state = state\n",
    "                # agent model inference\n",
    "                action = dqn.act_discrete_with_noise(\n",
    "                    {\"some_state\": old_state}\n",
    "                )\n",
    "                state, reward, terminal, _ = env.step(action.item())\n",
    "                state = t.tensor(state, dtype=t.float32).view(1, observe_dim)\n",
    "                total_reward += reward\n",
    "                \n",
    "                ep_list.append({\n",
    "                    \"state\": {\"some_state\": old_state},\n",
    "                    \"action\": {\"action\": action},\n",
    "                    \"next_state\": {\"some_state\": state},\n",
    "                    \"reward\": reward,\n",
    "                    \"terminal\": terminal or step == max_steps\n",
    "                })\n",
    "\n",
    "        dqn.store_episode(ep_list)\n",
    "        # update, update more if episode is longer, else less\n",
    "        if episode > 100:\n",
    "            for _ in range(step):\n",
    "                dqn.update()\n",
    "\n",
    "        # show reward\n",
    "        smoothed_total_reward = (smoothed_total_reward * 0.9 +\n",
    "                                    total_reward * 0.1)\n",
    "        logger.info(\"Episode {} total reward={:.2f}\"\n",
    "                    .format(episode, smoothed_total_reward))\n",
    "\n",
    "        if smoothed_total_reward > solved_reward:\n",
    "            reward_fulfilled += 1\n",
    "            if reward_fulfilled >= solved_repeat:\n",
    "                logger.info(\"Environment solved!\")\n",
    "                break\n",
    "        else:\n",
    "            reward_fulfilled = 0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a293d65eec8dde5de2981af85d84dd43bd8051257380932fcaaf2cf90c1085ab"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
